#!/usr/bin/env bash
# 
# This script starts a comdb2 cluster
# needs the following parameters:
# [optionally] a directory to store the database files, if ommited
#   it will use current working directory
# [optionally] a parameter to require ssl communication with the db
# the database name
# cluster nodes (at least two nodes)

#set -x

failexit() {
    echo $1
    exit -1
}

usage() {
    failexit "usage: $0 [--dir /path/to/dir] [--usessl] dbname node1 node2 [..nodeN]"
}


if [[ $# -lt 3 || $1 = "--help" || $1 = "-h" ]] ; then
    usage
fi

if [[ $1 = "--dir" ]] ; then 
    shift
    DBDIR=$1
    shift
fi

if [[ $1 = "--usessl" ]] ; then 
    USESSL=1
    shift
fi


DBNAME=$1
shift

#what remains is the cluster line
if [[ $# -lt 2 ]] ; then
    echo "Error: cluster needs to have at least two nodes."
    usage
fi

CLUSTER=$@

# use --dir to specify db directory otherwise creates db in current_dir/$DBNAME
if [[ -z $DBDIR ]]; then
    DBDIR=`pwd`/$DBNAME/
else
    DBDIR=`readlink -f ${DBDIR}`
fi

if [ "x$CLUSTER" = "x" ] ; then
    usage
fi


TMPDIR=${TMPDIR:-/tmp}
CDB2_CONFIG=$DBDIR/comdb2db.cfg
CDB2_OPTIONS="--cdb2cfg ${CDB2_CONFIG}"
COMDB2_EXE=/dev/shm/comdb2/comdb2
COMDB2AR_EXE=/dev/shm/comdb2/comdb2ar
export comdb2ar=$COMDB2AR_EXE  # used by copycomdb2
export COMDB2AR_EXOPTS="-x $COMDB2_EXE"
export COMDB2_ROOT=${DBDIR}
myhostname=`hostname`

check_executables_for_cluster() 
{
    for node in $CLUSTER; do
        if [ $node == $myhostname ] ; then
            readlink -f $DBDIR && failexit "$DBDIR already exists, please clean up before attempting to start"
            which $COMDB2_EXE > /dev/null || failexit "$COMDB2_EXE not in path"
            which copycomdb2  > /dev/null || failexit "copycomdb2 not in path"
            continue
        fi

        ssh -o StrictHostKeyChecking=no $node "ls $DBDIR &> /dev/null" < /dev/null && failexit "$DBDIR already exists in node $node, please clean up before attempting to start"
        ssh -o StrictHostKeyChecking=no $node "which $COMDB2_EXE > /dev/null" < /dev/null || failexit "$COMDB2_EXE not in path in node $node"
        ssh -o StrictHostKeyChecking=no $node "which $COMDB2AR_EXE > /dev/null" < /dev/null || failexit "$COMDB2AR_EXE not in path in node $node"
    done
}

check_executables_for_cluster


mkdir -p $DBDIR $TMPDIR

# setup files:
echo "$DBNAME: creating"
LRL="$DBDIR/$DBNAME.lrl"
> ${LRL}

if [[ -n $USESSL ]] ; then
    cat >> $DBDIR/$DBNAME.lrl <<EOPTIONS
ssl_client_mode REQUIRE
ssl_cert_path $DBDIR
EOPTIONS
fi

cat >> $DBDIR/$DBNAME.lrl <<EOPTIONS
name    $DBNAME
dir     $DBDIR

EOPTIONS

df $DBDIR | awk '{print $1 }' | grep "tmpfs\|nfs" > /dev/null && echo "setattr directio 0" >> ${LRL}

echo $DBNAME 0 $CLUSTER > $CDB2_CONFIG
echo "comdb2_config:default_type=testsuite" >> $CDB2_CONFIG
echo "cluster nodes $CLUSTER" >> $DBDIR/$DBNAME.lrl

for node in $CLUSTER; do #verify we can ssh to nodes
    if [ $node == $myhostname ] ; then
        continue
    fi
    hst=$(ssh -o StrictHostKeyChecking=no $node "hostname")
    if [ "x$hst" == "x" ] ; then
        failexit "cant get hostname on $node"
    fi
done

# Configure client SSL
echo "comdb2_config:ssl_cert_path=$DBDIR" >>$CDB2_CONFIG

set +e

pmux_port=5105
pmux_cmd='pmux -n'
if [ -n "$PMUXPORT" ] ; then
    pmux_port=$PMUXPORT
    pmux_socket=/tmp/pmux.socket.$PMUXPORT
    pmux_port_range="-r 21000:22000"
    pmux_cmd="pmux -n -p $PMUXPORT -b $pmux_socket $pmux_port_range"
    echo "comdb2_config:portmuxport=$PMUXPORT" >> $CDB2_CONFIG
    echo "portmux_port $PMUXPORT" >> ${LRL}
    echo "portmux_bind_path $pmux_socket" >> ${LRL}
fi
stop_pmux="pgrep pmux > /dev/null && (exec 3<>/dev/tcp/localhost/${pmux_port} && echo exit >&3 )"




keygen() {
# Setup ssl certificate
    openssl req -x509 -newkey rsa:4096 -keyout ${DBDIR}/server.key \
        -out ${DBDIR}/server.crt \
        -days 365 -nodes -subj \
        "/C=US/ST=New York/L=New York/O=Bloomberg/OU=Comdb2/CN=*.bloomberg.com" &>> $DBDIR/setup.log
    cp ${DBDIR}/server.crt ${DBDIR}/root.crt
    chmod 600 ${DBDIR}/server.key
    cp ${DBDIR}/server.crt ${DBDIR}/client.crt
    cp ${DBDIR}/server.key ${DBDIR}/client.key
}


copy_files_to_cluster() 
{
    # copy executables to each node except localhost
    for node in $CLUSTER; do
        if [ $node == $myhostname ] ; then
            continue
        fi

        ssh -o StrictHostKeyChecking=no $node "mkdir -p $DBDIR/logs/ $DBDIR/var/log/cdb2 $DBDIR/tmp/cdb2" < /dev/null
        ssh -o StrictHostKeyChecking=no $node "$pmux_cmd" &>> $DBDIR/setup.log < /dev/null
        if [[ -n $USESSL ]] ; then
            scp -o StrictHostKeyChecking=no ${DBDIR}/server.key ${DBDIR}/server.crt $node:${DBDIR} &>> $DBDIR/setup.log
            ssh -o StrictHostKeyChecking=no $node "cp ${DBDIR}/server.crt ${DBDIR}/root.crt || chmod 600 ${DBDIR}/server.key"
        fi
    done
}


#generate ssl keys for the db
if [[ -n $USESSL ]] ; then
    keygen
fi
copy_files_to_cluster


for csc2 in $(ls *.csc2 2>/dev/null); do
    table=${csc2%%.csc2}
    cp $PWD/$csc2 $DBDIR/
done >> $DBDIR/${DBNAME}.lrl

mkdir -p $DBDIR/logs $DBDIR/var/log/cdb2 $DBDIR/tmp/cdb2

$COMDB2_EXE --create --lrl $DBDIR/${DBNAME}.lrl --pidfile ${TMPDIR}/$DBNAME.pid $DBNAME  >$DBDIR/logs/${DBNAME}.init 2>&1
rc=$?
rm -f ${DBNAME}.trap
if [[ $rc -ne 0 ]]; then
    echo "Error rc=$rc while initializing DB, see $TESTDIR/logs/${DBNAME}.init "
    exit 1
fi
echo "${DBNAME} created successfully"

set -e  # from here, a bad rc will mean failure and exit


# start it
echo "$DBNAME: copying to cluster"
for node in $CLUSTER; do
    if [ $node == $myhostname ] ; then # skip copying to ourself
        continue
    fi
    copycomdb2 $DBDIR/${DBNAME}.lrl ${node}: &> $DBDIR/logs/${DBNAME}.${node}.copy
    if [[ $? -ne 0 ]]; then
        echo "FAILED: copycomdb2 $DBDIR/${DBNAME}.lrl ${node}: "
        echo "see $DBDIR/logs/${DBNAME}.${node}.copy "
        exit 1
    fi
done

echo "export COMDB2_ROOT=$DBDIR" >> ${DBDIR}/replicant_vars
echo "export PATH=$PATH" >> ${DBDIR}/replicant_vars
CMD="source ${DBDIR}/replicant_vars ; $COMDB2_EXE ${DBNAME} -lrl $DBDIR/${DBNAME}.lrl"
echo "$DBNAME: starting"
for node in $CLUSTER; do
    if [ $node == $myhostname ] ; then # dont ssh to ourself -- just start db locally
        $COMDB2_EXE ${DBNAME} -lrl $DBDIR/${DBNAME}.lrl -pidfile ${TMPDIR}/${DBNAME}.${node}.pid 2>&1 | gawk '{ print strftime("%H:%M:%S>"), $0; fflush(); }' >$DBDIR/logs/${DBNAME}.${node}.db 2>&1 &
        continue
    fi

    scp -o StrictHostKeyChecking=no ${DBDIR}/replicant_vars $node:${DBDIR}/replicant_vars &>> $DBDIR/setup.log
    # redirect output from CMD to a subshell which runs awk to prepend time
    ssh -o StrictHostKeyChecking=no -tt $node ${CMD} 2>&1 </dev/null > >(gawk '{ print strftime("%H:%M:%S>"), $0; fflush(); }' > $DBDIR/logs/${DBNAME}.${node}.db)  &
    # $! will be pid of ssh (if we had used pipe, $! would be pid of awk)
    echo $! > ${TMPDIR}/${DBNAME}.${node}.pid
done

set +e
echo "$DBNAME: waiting until ready"
sleep 1
for node in $CLUSTER; do
    out=$(cdb2sql ${CDB2_OPTIONS} --tabs --host $node $DBNAME 'select 1' 2>&1)
    while  [[ "$out" != "1" ]]; do
        sleep 2
        out=$(cdb2sql ${CDB2_OPTIONS} --tabs --host $node $DBNAME 'select 1' 2>&1)
    done
done

n=$(cdb2sql ${CDB2_OPTIONS} -tabs $DBNAME --host $node 'exec procedure sys.cmd.send("bdb cluster")' | grep "lsn" | wc -l)
echo "$DBNAME: Cluster is running with $n nodes. You can query it with:"
echo "   cdb2sql ${CDB2_OPTIONS} $DBNAME default 'select 1'"

exit 0
