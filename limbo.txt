The database replicant cored with this stack:

#0  0x00007f432a42c428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007f432a42e02a in __GI_abort () at abort.c:89
#2  0x0000000000830c74 in __db_pg_alloc_recover (dbenv=0x2f51a18, dbtp=0x7f42ffa354c0, lsnp=0x7f42ffa35430, op=DB_TXN_APPLY,
        info=0x39cf170) at ../berkdb/db/db_rec.c:815
#3  0x000000000081807d in __db_dispatch (dbenv=0x2f51a18, dtab=0x7f4306de1390, dtabsize=182, db=0x7f42ffa354c0, lsnp=0x7f42ffa35430,
        redo=DB_TXN_APPLY, info=0x39cf170) at ../berkdb/db/db_dispatch.c:715
#4  0x00000000008bb0aa in __rep_process_txn_int (dbenv=0x2f51a18, rctl=0x3e3e040, rec=0x7f42ffa35aa0, ltrans=0x7f42ffa35720,
        maxlsn=..., commit_gen=0x869e770, lockid=993, rp=0x0, lcin=0x7f42ffa354a0) at ../berkdb/rep/rep_record.c:4864
#5  0x00000000008bb61b in __rep_process_txn (dbenv=0x2f51a18, rctl=0x3e3e040, rec=0x7f42ffa35aa0, ltrans=0x7f42ffa35720, maxlsn=...,
        commit_gen=0x869e770) at ../berkdb/rep/rep_record.c:4994
#6  0x00000000008b765a in __rep_apply_int (dbenv=0x2f51a18, rp=0x3e3e040, rec=0x7f42ffa35aa0, ret_lsnp=0x7f42ffa35a30,
        commit_gen=0x869e770, decoupled=1) at ../berkdb/rep/rep_record.c:3627
#7  0x00000000008b7b61 in __rep_apply (dbenv=0x2f51a18, rp=0x3e3e040, rec=0x7f42ffa35aa0, ret_lsnp=0x7f42ffa35a30,
        commit_gen=0x869e770, decoupled=1) at ../berkdb/rep/rep_record.c:3769
#8  0x00000000008adc3f in apply_thread (arg=0x2f51a18) at ../berkdb/rep/rep_record.c:561
#9  0x00007f432a7c86ba in start_thread (arg=0x7f42ffa36700) at pthread_create.c:333
#10 0x00007f432a4fe41d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109


This trace came from the database:
2020/03/28 14:22:33 Log sequence error at 10:4305622 : page LSN 1:0; previous LSN 9:40692687
pgdump> __dbreg_id_to_db -1 error=2

The log record in question:

[10][4305622]__db_pg_alloc: rec: 1049 txnid 80000013 prevlsn [10][4305550]
     fileid: -1
     ufid_fileid:
     0:C3002C00 00080000 57957F5E 48C93800   |..,.....W..^H.8.|
    10:00000000                              |....            |
     meta_lsn: [10][4257315]
     meta_pgno: 0
     page_lsn: [9][40692687]
     pgno: 51
     ptype: 5
     next: 0
     
This shows that pgno 51 should have a page-LSN of [9][40692687], but according 
to the error message and the corefile, this page has an LSN of [1][0].  What's
interesting is that [9][40692687] describes a pgfree from a compensating 
transaction:

[9][40692687]__db_pg_free: rec: 1050 txnid 8000005a prevlsn [0][0]
    fileid: -1
    ufid_fileid:
    0:C3002C00 00080000 57957F5E 48C93800  |..,.....W..^H.8.|
   10:00000000                             |....            |
    pgno: 51
    meta_lsn: [0][1]
    meta_pgno: 0
    header:
    0:00000000 00000000 00000033 00000000  |...........3....|
   10:00000000 00001000 00000000 00000000  |................|
    next: 0

This committed successfully
[9][40697887]__txn_regop_gen: rec: 16 txnid 8000005a prevlsn [9][40697783]
    opcode: 1
    generation: 12
    context: 00000af113000000 00000013f10a0000
    timestamp: 1585419731 (Sat Mar 28 14:22:11 2020, 202003281422.11)
    locks:
        .. <list of locks> ..

This transaction was generated by the new master- so the replicant in processing
the redo-log for this transaction should have updated its page-LSN to this
log-record, [9][40692687] - the value emitted later in the failing log-record.

>> This is an aside ..
(
If we do a pagedump of the file in question, we see that there are only two
pages that are part of the filesystem (pages 0 and 1).  This makes sense, 
because the memp_fget code doesn't falloc in response to a DB_MPOOL_CREATE (we 
just do a pagewrite, and accept the fact that we can have "holes").  We already
have the lsnerr_logflush berkley attribute.  I've added another which syncs
the bufferpool, lsnerr_mempsync.
)


__db_pg_free_recover
    _memcpy(&copy_lsn, &LSN(argp->header.data), sizeof(DB_LSN));
    cmp_n = log_compare(lsnp, &LSN(pagep));
    cmp_p = log_compare(&LSN(pagep), &copy_lsn);
    CHECK_LSN(op, cmp_p, &LSN(pagep), &copy_lsn, lsnp, argp->fileid,
            argp->pgno);
    
    if (DB_REDO(op) && (cmp_p == 0 || (IS_ZERO_LSN(copy_lsn) &&
                    log_compare(&LSN(pagep), &argp->meta_lsn) <= 0))) {
         /* Need to redo update described. */
            P_INIT(pagep, file_dbp->pgsize,
                argp->pgno, PGNO_INVALID, argp->next, 0, P_INVALID);
            pagep->lsn = *lsnp;
    
            modified = 1;
    }
    
When the master emitted the log, the page-LSN was 0 (you can see in the header
of the log record).  It turns out that when this log record was emitted, the
page-LSN on this machine was ALREADY [1][0].  This is larger than the 
[0][0] described in the log, and as the meta_lsn happens to be [0][1] in this 
case (which is smaller than [1][0], it wouldn't have been applied.

It turns that page-LSN [1][0] is special: it is the LSN which is used for
a page when DB_LIMBO_RECOVER puts a page on the free-list unlogged.  This is 
broken behavior, and was the direct cause of the crash.  

1
A master was elected (actually, the same master was re-elected).  The crashing 
node did a rep-verify match, and ran recovery.  It's backwards pass (which 
collects pages to place on the freelist) begins in the middle of a transaction
generated by the new (actually same) master.

2
At the end of recovery this node did limbo processing, and placed several 
pages on the freelist.  It did not log these changes, but each page was given
a page-LSN of [1][0].

3
The new master aborts this transaction, but then does a limbo-compensate to
place the newly allocated pages into the freelist, and to generate logs for
these.

4
Because the page-LSN was too high ([1][0] rather than [0][0]), the abort wasn't
applied.  If the abort had been applied, we should have seen corruption, as the
code would have attempted to place a page in the freelist which was already in
the freelist.

This is "step 1": it disables the code that can corrupt the freelist.
